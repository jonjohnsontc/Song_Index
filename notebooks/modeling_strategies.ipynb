{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Strategies for Songwriter Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.preprocessing import StandardScaler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining strategies to use in feature selection & engineering when building the songwriter recommender.\n",
    "\n",
    "Current thought about ideating through the dataset is looking at each \"chunk\" of information that I have, and seeing specifically how I can engineer each portion\n",
    "\n",
    "## Incorporating Audio Analysis\n",
    "\n",
    "The audio analysis endpoint is made up of an assortment of distinct datasets within a single object:\n",
    "\n",
    "|Key|Value Type|Value Description|\n",
    "|---|---|---|\n",
    "|bars|an array of time interval objects|The time intervals of the bars throughout the track. A bar (or measure) is a segment of time defined as a given number of beats. Bar offsets also indicate downbeats, the first beat of the measure|\n",
    "|beats|an array of time interval objects|The time intervals of beats throughout the track. A beat is the basic time unit of a piece of music; for example, each tick of a metronome. Beats are typically multiples of tatums.|\n",
    "|sections|an array of section objects|Sections are defined by large variations in rhythm or timbre, e.g. chorus, verse, bridge, guitar solo, etc. Each section contains its own descriptions of tempo, key, mode, time_signature, and loudness.|\n",
    "segments|an array of segment objects|Audio segments attempts to subdivide a song into many segments, with each segment containing a roughly consitent sound throughout its duration.|\n",
    "tatums|an array of time interval objects|A tatum represents the lowest regular pulse train that a listener intuitively infers from the timing of perceived musical events (segments). For more information about tatums, see Rhythm (below).|\n",
    "\n",
    "So essentially, we're looking at **time interval objects**, **section objects**, and **segment objects**.\n",
    "\n",
    "### Potential Ideas\n",
    "\n",
    "#### SVD\n",
    "\n",
    "Performing SVD on segment objects per song, perhaps limiting each to 50 components. \n",
    "\n",
    "Grabbing the Mean and Variance of tatum duration per song\n",
    "\n",
    "Afterwards, performing SVD again on each of the songs tatum and segment objects tied to a single writer (no weighting applied for writing percentage). And then capturing the other dimensions with some other sort of aggregation:\n",
    "\n",
    "- Genres?\n",
    "- Key?\n",
    "\n",
    "With each of the below, perhaps I can just use the mean and variance of all songs tied to a particular writer:\n",
    "\n",
    "- Tempo (perhaps mean & variance?)\n",
    "- Mode (perhaps just mean)\n",
    "- Time Signature?\n",
    "- Danceability?\n",
    "- Energy?\n",
    "- Valence?\n",
    "- Speechiness?\n",
    "- Acousticness?\n",
    "\n",
    "##### Pros\n",
    "- Seems like it would capture the necessary latent factors that make a song a song.\n",
    "\n",
    "##### Cons\n",
    "- Less interpretable, then say, computing the mean and variance of those values\n",
    "\n",
    "#### Testing SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/jonjohnson/../../Volumes/gdrive/SWI_data/data/new_tracks_20190103.json', 'r') as f:\n",
    "    tracks = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "651517"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_to_sample = random.sample(tracks.keys(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4RQMYkHQrEiwvbKtlaW2w8',\n",
       " '3cjQIgkhmxnahbeoDmiyFe',\n",
       " '58jyyd5QoWnT9Cc3q3ozLG',\n",
       " '4naeJLeZDIHxV8hsvU220v',\n",
       " '5H2j7EbqkqvbOBwacvo0Vu',\n",
       " '4TXGv208zoONRBIkDtzp68',\n",
       " '1EkjxpyMHq6iP6ASzrbCpi',\n",
       " '4QWrHRsIaMixg0YD9yfMAw',\n",
       " '2IziixGyxzwsjX54CdfrNk',\n",
       " '5RuotSKQoklW1NER0fuOCn']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_to_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_json(lst):\n",
    "    track_dict = {}\n",
    "    for i in lst:\n",
    "        with open(f'/Users/jonjohnson/../../Volumes/gdrive/SWI_data/data/audio_analysis/{i}.json', 'r') as f:\n",
    "            track = json.load(f)\n",
    "        track_dict[i] = track\n",
    "    return track_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_dict = retreive_json(tracks_to_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.29 , 0.137, 0.159, 0.166, 0.229, 0.478, 0.461, 0.081, 0.148,\n",
       "        0.371, 1.   , 0.926])]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hsplit(np.array(tracks_dict['4RQMYkHQrEiwvbKtlaW2w8']['segments'][0]['pitches']), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting Pitch and Timbre Values for each Song "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch_timbre_grabber(dct):\n",
    "    pitches = np.hsplit(np.array(dct['segments'][0]['pitches']), 1)\n",
    "    timbre = np.hsplit(np.array(dct['segments'][0]['pitches']), 1)\n",
    "    for i in range(1, len(dct['segments'])):\n",
    "        pitches = np.hstack([pitches, np.hsplit(np.array(dct['segments'][i]['pitches']), 1)])\n",
    "        timbre = np.hstack([timbre, np.hsplit(np.array(dct['segments'][i]['timbre']), 1)])\n",
    "    return pitches, timbre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,t = pitch_timbre_grabber(tracks_dict['4RQMYkHQrEiwvbKtlaW2w8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(50, random_state=333)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pitch vector is transposed below, because the number of components must be less than the number of features (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components=50 must be between 0 and min(n_samples, n_features)=1 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-47e8e8bc7289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;31m# Call different fits for either full or truncated SVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'full'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'arpack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'randomized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    423\u001b[0m                              \u001b[0;34m\"min(n_samples, n_features)=%r with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m                              \u001b[0;34m\"svd_solver='full'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                              % (n_components, min(n_samples, n_features)))\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_components=50 must be between 0 and min(n_samples, n_features)=1 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "test_x = pca.fit_transform(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44.08682634]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing What I Did Before\n",
    "\n",
    "Building a pipeline with how I processed songs before. Here's what I need:\n",
    "\n",
    "- Record key changes for each song section\n",
    "- One-Hot Encoded Genres \n",
    "  - Then apply SVD\n",
    "  - This should be easy; There shouldn't be any new artists, so I should be able to apply what I've got on a song level\n",
    "- Gathering Mean & Variance of Audio Analysis Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
